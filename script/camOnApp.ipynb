{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from PyQt5.QtWidgets import QApplication, QMainWindow, QPushButton, QWidget, QLabel, QGridLayout, QTextEdit, QMessageBox, QFileDialog\n",
    "from PyQt5.QtGui import QFont, QImage, QPixmap\n",
    "from PyQt5.QtCore import Qt\n",
    "import numpy as np\n",
    "import sys\n",
    "import cv2\n",
    "import time\n",
    "import os\n",
    "import dlib\n",
    "from imutils import face_utils\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.applications.resnet_v2 import ResNet50V2\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from tensorflow import keras\n",
    "\n",
    "K = tf.keras.backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \".\"\n",
    "data_dir = '../created_profiles/'\n",
    "data_dir_out = \"../created_profiles_dlib/\"\n",
    "\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "\n",
    "if not os.path.exists(data_dir_out):\n",
    "    os.makedirs(data_dir_out)\n",
    "\n",
    "nameToAdd = \"\"\n",
    "person_id_make_data = 0\n",
    "list_of_person = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRidOfExtenstion(fileName): #Renvoie le nom d'un fichier sans son extension.\n",
    "    return fileName.split('.')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCroppedFace(image_path): #Renvoie l'image du visage d'une personne sur l'image entrée en paramètre.\n",
    "    face_detector = dlib.get_frontal_face_detector()\n",
    "    image = cv2.imread(image_path)\n",
    "    BGR2RGB = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    faces = face_detector(BGR2RGB, 0)\n",
    "    if len(faces) == 1:\n",
    "        for face in faces:\n",
    "            face_bounding_box = face_utils.rect_to_bb(face)\n",
    "            if all(i >= 0 for i in face_bounding_box):\n",
    "                [x, y, w, h] = face_bounding_box\n",
    "                frame = BGR2RGB[y:y + h, x:x + w]\n",
    "                frame = cv2.resize(frame, (224, 224))\n",
    "                return frame\n",
    "    else:\n",
    "        return 0\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addImage(person_ID, person_NAME, img_ID, face_IMG_file_name, dataset): #Ajoute un fichier et toutes ses informations au dataframe. (Insère une ligne)\n",
    "    new_row = {\n",
    "        'person_id': person_ID,\n",
    "        'person_name': person_NAME,\n",
    "        'img_id': img_ID,\n",
    "        'face_img_file_name': face_IMG_file_name\n",
    "        }\n",
    "    dataset = dataset.append(new_row, ignore_index=True)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_faces = { #Colonnes du dataframe\n",
    "    'person_id': [],\n",
    "    'person_name': [],\n",
    "    'img_id': [],\n",
    "    'face_img_file_name': []\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data(folder_path, person_id_make_data, name, data): #Crée le dataframe\n",
    "\n",
    "    dataframe = pd.DataFrame(data)\n",
    "    person_id_link = []\n",
    "    \n",
    "    persons = os.listdir(folder_path)\n",
    "    \n",
    "    images_files_names = os.listdir(folder_path)\n",
    "    \n",
    "    id_name_tuple = (person_id_make_data, name)\n",
    "    person_id_link.append(id_name_tuple)\n",
    "    \n",
    "    for image_index in range(len(images_files_names)):\n",
    "        img_file_name = images_files_names[image_index]\n",
    "        image_id = int(img_file_name.split('_')[-1].split('.')[0])\n",
    "        face_image_file_name = folder_path + \"/\" + persons[image_index]\n",
    "        \n",
    "        dataframe = addImage(\n",
    "            person_ID = person_id_make_data,\n",
    "            person_NAME = name,\n",
    "            img_ID = image_id,\n",
    "            face_IMG_file_name = face_image_file_name,\n",
    "            dataset = dataframe\n",
    "        )\n",
    "    person_id_make_data += 1\n",
    "    dataframe = dataframe.astype({'person_id': 'int32'})\n",
    "    dataframe = dataframe.astype({'img_id': 'int32'})\n",
    "    list_of_person.append(name)\n",
    "    dataframe.to_csv(\"df_faces.csv\", index=False, header=True)\n",
    "    return dataframe, person_id_make_data, list_of_person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def treatPictures(name):\n",
    "    total_pictures = 0\n",
    "    detected_faces = 0\n",
    "    pathToFolder = \"../created_profiles/\" + name\n",
    "    images_files = os.listdir(pathToFolder) #Liste de toutes les images.jpg de cette personne.\n",
    "    number_of_images = len(images_files)\n",
    "    face_images = []\n",
    "    face_images_files_names = []\n",
    "    for image_number in range(number_of_images):\n",
    "        total_pictures += 1\n",
    "        image_file_name = str(images_files[image_number])\n",
    "        image_real_number = image_file_name.split(' ')[-1].split('.')[0] #On recupere le numero de l'image à partir du nom du fichier.\n",
    "        image_path = pathToFolder + '/' + image_file_name\n",
    "        cropped_face = getCroppedFace(image_path) #On récupère le visage sur la photo.\n",
    "        \n",
    "        if type(cropped_face) != int: #Si un seul visage a été trouvé sur la photo.\n",
    "            detected_faces += 1\n",
    "            face_images.append(cropped_face)\n",
    "            face_image_file_name = name + \"_\" + image_real_number + \".jpg\"\n",
    "            face_images_files_names.append(face_image_file_name)\n",
    "    \n",
    "    if len(face_images) != 0: #Si au moins un visage a été détecté sur toutes les photos de la personne.\n",
    "        path_face_folder = \"../created_profiles_dlib/\" + name\n",
    "        os.mkdir(path_face_folder) #Création du dossier qui va contenir les photos de tous ses visages.\n",
    "        for face_image_index in range(len(face_images)):\n",
    "            path_to_write = path_face_folder + \"/\" + face_images_files_names[face_image_index]\n",
    "            try:\n",
    "                cv2.imwrite(path_to_write, cv2.cvtColor(face_images[face_image_index], cv2.COLOR_RGB2BGR)) #Création du fichier .jpg.\n",
    "            except:\n",
    "                pass\n",
    "    message = str(detected_faces) + \" faces detected on \" + str(total_pictures) + \" pictures.\"\n",
    "    shutil.rmtree(pathToFolder)\n",
    "    return message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gray_image(image): #Renvoie l'image d'entrée en niveau de gris\n",
    "    return cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_images(list_of_images): #Normalise les images\n",
    "    l = len(list_of_images)\n",
    "    for i in range(l):\n",
    "        list_of_images[i] = list_of_images[i]/255.\n",
    "    return list_of_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(X, y, gray): #Rend les données utilisables par le modèle. gray ---> booléen, True si on veut les images en niveau de gris, False pour RGB.\n",
    "    if gray:\n",
    "        for i in range(len(X)):\n",
    "            X[i] = gray_image(X[i])\n",
    "        X = np.asarray(X)\n",
    "        X = X/255.\n",
    "        X = X.reshape(X.shape[0], 224, 224, 1)\n",
    "    else:\n",
    "        X = normalize_images(X)\n",
    "        X = np.asarray(X)\n",
    "        X = X.reshape(X.shape[0], 224, 224, 3)\n",
    "    y = to_categorical(y)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(df): #Entrée: dataframe et chemin du dossier data. Renvoie X (les images) et y (les labels).\n",
    "    IDs = list(df['person_id'])\n",
    "    FILEs = list(df['face_img_file_name'])\n",
    "    number_of_images = len(IDs)\n",
    "\n",
    "    print(IDs)\n",
    "    print(FILEs)\n",
    "    print(number_of_images)\n",
    "    \n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    for index in range(number_of_images):\n",
    "        ID = int(IDs[index])\n",
    "        file_path = FILEs[index]\n",
    "        image = plt.imread(file_path)\n",
    "        X.append(image)\n",
    "        y.append(ID)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet_model_tf(input_shape, nombre_classes):\n",
    "    resnet = ResNet50V2(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    resnet.tbatch_sizenable = False\n",
    "    model = Sequential()\n",
    "    model.add(resnet)\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(nombre_classes, activation='softmax'))\n",
    "    \n",
    "    print(model.summary())\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', \n",
    "                                            patience=2, \n",
    "                                            verbose=1, \n",
    "                                            factor=0.7, \n",
    "                                            min_lr=0.00000000001)\n",
    "    return model, learning_rate_reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_input(x, data_format=None, version=1):\n",
    "    x_temp = np.copy(x)\n",
    "    if data_format is None:\n",
    "        data_format = K.image_data_format()\n",
    "    assert data_format in {'channels_last', 'channels_first'}\n",
    "\n",
    "    if version == 1:\n",
    "        if data_format == 'channels_first':\n",
    "            x_temp = x_temp[:, ::-1, ...]\n",
    "            x_temp[:, 0, :, :] -= 93.5940\n",
    "            x_temp[:, 1, :, :] -= 104.7624\n",
    "            x_temp[:, 2, :, :] -= 129.1863\n",
    "        else:\n",
    "            x_temp = x_temp[..., ::-1]\n",
    "            x_temp[..., 0] -= 93.5940\n",
    "            x_temp[..., 1] -= 104.7624\n",
    "            x_temp[..., 2] -= 129.1863\n",
    "\n",
    "    elif version == 2:\n",
    "        if data_format == 'channels_first':\n",
    "            x_temp = x_temp[:, ::-1, ...]\n",
    "            x_temp[:, 0, :, :] -= 91.4953\n",
    "            x_temp[:, 1, :, :] -= 103.8827\n",
    "            x_temp[:, 2, :, :] -= 131.0912\n",
    "        else:\n",
    "            x_temp = x_temp[..., ::-1]\n",
    "            x_temp[..., 0] -= 91.4953\n",
    "            x_temp[..., 1] -= 103.8827\n",
    "            x_temp[..., 2] -= 131.0912\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    return x_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MainWindow(QMainWindow): #Fenetre principale\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.initUI()\n",
    "        self.camActivated = False\n",
    "    \n",
    "    def fit_model(self,df_faces,person_id_make_data):\n",
    "        name = os.listdir(data_dir_out)\n",
    "        if (len(name) != 0):\n",
    "            for e in name :\n",
    "                df_faces, person_id_make_data, list_of_person = make_data(data_dir_out + e, person_id_make_data, e, df_faces)\n",
    "            X, y = get_data(df_faces)\n",
    "\n",
    "            gray_images = False # Mettre True pour avoir les images en niveau de gris, False pour RGB\n",
    "            X, y = preprocess_data(X, y, gray_images)\n",
    "\n",
    "            cnn_model, learning_rate_reduction= resnet_model_tf((224, 224, 3), len(os.listdir(data_dir_out)))\n",
    "            cnn_model.fit(X, y, epochs=10, verbose=1, callbacks=[learning_rate_reduction])\n",
    "            cnn_model.save(\"cnn_model\")\n",
    "        else :\n",
    "            self.popup = QMessageBox(QMessageBox.Information,'Message',\"Cannot fit\")\n",
    "            self.popup.show()\n",
    "    \n",
    "    def initUI(self):\n",
    "        self.setWindowTitle('Camera')\n",
    "        \n",
    "        self.layout = QGridLayout()\n",
    "        \n",
    "        self.screen = QLabel(\"\")\n",
    "        self.screen.setMinimumHeight(640)\n",
    "        self.screen.setMinimumWidth(800)\n",
    "        self.screen.setMaximumHeight(640)\n",
    "        self.screen.setMaximumWidth(800)\n",
    "        self.screen.setStyleSheet('background-color: black')\n",
    "        self.layout.addWidget(self.screen, 0, 0, 1, 2)\n",
    "        \n",
    "        self.camButton = QPushButton(\"Activate Camera\")\n",
    "        self.camButton.setMinimumHeight(70)\n",
    "        self.camButton.setMinimumWidth(200)\n",
    "        self.camButton.setMaximumHeight(70)\n",
    "        self.camButton.setMaximumWidth(250)\n",
    "        self.camButton.setStyleSheet('background-color: darkgreen')\n",
    "        self.camButton.setFont(QFont('Arial Black', 15))\n",
    "        self.camButton.clicked.connect(self.camInteraction)\n",
    "        self.layout.addWidget(self.camButton, 1, 0)\n",
    "        \n",
    "        self.takePicButton = QPushButton(\"Take Pictures\")\n",
    "        self.takePicButton.setMinimumHeight(70)\n",
    "        self.takePicButton.setMinimumWidth(250)\n",
    "        self.takePicButton.setMaximumHeight(70)\n",
    "        self.takePicButton.setMaximumWidth(250)\n",
    "        self.takePicButton.setStyleSheet('background-color: grey')\n",
    "        self.takePicButton.setFont(QFont('Arial Black', 15))\n",
    "        self.takePicButton.clicked.connect(self.takePictures)\n",
    "        self.layout.addWidget(self.takePicButton, 1, 1)\n",
    "        \n",
    "        self.fileButton = QPushButton(\"File Explorer\")\n",
    "        self.fileButton.setMinimumHeight(70)\n",
    "        self.fileButton.setMinimumWidth(250)\n",
    "        self.fileButton.setMaximumHeight(70)\n",
    "        self.fileButton.setMaximumWidth(250)\n",
    "        self.fileButton.setStyleSheet('background-color: darkgreen')\n",
    "        self.fileButton.setFont(QFont('Arial Black', 15))\n",
    "        self.fileButton.clicked.connect(self.open_dialog_box)\n",
    "        self.layout.addWidget(self.fileButton, 1, 2)\n",
    "\n",
    "        self.fileButton = QPushButton(\"Fit the model\")\n",
    "        self.fileButton.setMinimumHeight(70)\n",
    "        self.fileButton.setMinimumWidth(794)\n",
    "        self.fileButton.setMaximumHeight(70)\n",
    "        self.fileButton.setMaximumWidth(794)\n",
    "        self.fileButton.setStyleSheet('background-color: red')\n",
    "        self.fileButton.setFont(QFont('Arial Black', 15))\n",
    "        self.fileButton.clicked.connect(lambda:self.fit_model(df_faces,person_id_make_data))\n",
    "        self.layout.addWidget(self.fileButton, 3, 0)\n",
    "        \n",
    "        self.nameZone = QTextEdit(\"\")\n",
    "        self.nameZone.setMinimumHeight(47)\n",
    "        self.nameZone.setMinimumWidth(800)\n",
    "        self.nameZone.setMaximumHeight(47)\n",
    "        self.nameZone.setMaximumWidth(800)\n",
    "        self.nameZone.setStyleSheet('background-color: lightgrey')\n",
    "        self.nameZone.setFont(QFont('Arial', 18))\n",
    "        self.layout.addWidget(self.nameZone, 2, 0, 1, 2)\n",
    "        self.nameZone.setVisible(False)\n",
    "        \n",
    "        self.widget = QWidget()\n",
    "        self.widget.setLayout(self.layout)\n",
    "        self.setCentralWidget(self.widget)\n",
    "        \n",
    "        self.setFixedSize(822, 800)\n",
    "        self.move(700, 100)\n",
    "        \n",
    "    def camInteraction(self):\n",
    "        if not self.camActivated:\n",
    "            self.screen.setStyleSheet('background-color: grey')\n",
    "            self.camButton.setStyleSheet('background-color: darkred')\n",
    "            self.camButton.setText(\"Deactivate Camera\")\n",
    "            self.takePicButton.setStyleSheet('background-color: forestgreen')\n",
    "            self.nameZone.setVisible(True)\n",
    "            self.camActivated = True\n",
    "            self.cap = cv2.VideoCapture(1) ###\n",
    "            \n",
    "            face_detector = dlib.get_frontal_face_detector()\n",
    "\n",
    "            if os.path.exists(\"cnn_model\"):\n",
    "                cnn_model = keras.models.load_model('cnn_model')\n",
    "                check_model = True\n",
    "            else :\n",
    "                cnn_model = Sequential()\n",
    "                check_model = False\n",
    "\n",
    "            list_of_person = os.listdir(data_dir_out)\n",
    "            \n",
    "            while self.camActivated == True:\n",
    "                ret, cv_img = self.cap.read()\n",
    "\n",
    "                ##################################################### PARTIE DETECTION DE VISAGE\n",
    "                \n",
    "                gray = cv2.cvtColor(cv_img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "                face_detector = dlib.get_frontal_face_detector()\n",
    "                faces = face_detector(gray, 0)\n",
    "\n",
    "                for face in faces:\n",
    "                    face_bounding_box = face_utils.rect_to_bb(face)\n",
    "                    if all(i >= 0 for i in face_bounding_box):\n",
    "                        [x, y, w, h] = face_bounding_box\n",
    "                        frame = cv_img[y:y + h, x:x + w]\n",
    "                        cv2.rectangle(cv_img, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "                        frame = cv2.resize(frame, (224, 224))\n",
    "                        frame = np.asarray(frame, dtype=np.float64)\n",
    "                        frame = np.expand_dims(frame, axis=0)\n",
    "                        # frame = preprocess_input(frame)\n",
    "\n",
    "                        if (check_model):\n",
    "                            prediction = cnn_model.predict(frame)\n",
    "                            \n",
    "                            max_value = max(prediction[0])\n",
    "\n",
    "                            index = prediction[0].tolist().index(max_value)\n",
    "\n",
    "                            print(max_value)\n",
    "                            if (max_value > 0.7):\n",
    "                                name = list_of_person[index]\n",
    "                            else :  \n",
    "                                name = \"Unknown\"\n",
    "                            \n",
    "                            font_face = cv2.FONT_HERSHEY_SIMPLEX\n",
    "                            cv2.putText(cv_img, name, (x, y-5), font_face, 0.8, (0,0,255), 3)\n",
    "\n",
    "                ################################################################################\n",
    "                frame = cv2.cvtColor(cv_img, cv2.COLOR_BGR2RGB)\n",
    "                qimage = QImage(frame, frame.shape[1], frame.shape[0], QImage.Format_RGB888)\n",
    "                pixmap = QPixmap(qimage)\n",
    "                pixmap = pixmap.scaled(960,640, Qt.KeepAspectRatio)\n",
    "                self.screen.setPixmap(pixmap)\n",
    "                cv2.waitKey(1)\n",
    "            self.cap.release()\n",
    "        else:\n",
    "            im_np = np.zeros((960,540,1))\n",
    "            qimage = QImage(im_np, im_np.shape[1], im_np.shape[0], QImage.Format_RGB888)\n",
    "            pixmap = QPixmap(qimage)\n",
    "            pixmap = pixmap.scaled(640,400, Qt.KeepAspectRatio)\n",
    "            self.screen.setPixmap(pixmap)\n",
    "            self.screen.setStyleSheet('background-color: black')\n",
    "            self.camButton.setStyleSheet('background-color: darkgreen')\n",
    "            self.takePicButton.setStyleSheet('background-color: grey')\n",
    "            self.camButton.setText(\"Activate Camera\")\n",
    "            self.nameZone.setVisible(False)\n",
    "            self.camActivated = False\n",
    "    \n",
    "    \n",
    "    def open_dialog_box(self):\n",
    "        file_name , _ = QFileDialog.getOpenFileName(self, \"Select a file...\", directory=\"../created_profiles_dlib\")\n",
    "\n",
    "        image = Image.open(file_name)\n",
    "        image.show()        \n",
    "        \n",
    "    def takePictures(self):\n",
    "        if self.camActivated:\n",
    "            name = self.nameZone.toPlainText()\n",
    "            if name == \"\":\n",
    "                self.popup = QMessageBox(QMessageBox.Information,'Message','Please enter your name first.')\n",
    "                self.popup.show()\n",
    "            else:\n",
    "                self.nameZone.setText(\"\")\n",
    "                folders = os.listdir(\"../created_profiles/\")\n",
    "                if name not in folders:\n",
    "                    os.mkdir(\"../created_profiles/\" + name)\n",
    "                    count = 0\n",
    "                    total = 0\n",
    "                    timeBase = time.time()\n",
    "                    while total < 10:\n",
    "                        retCatch, frameCatch = self.cap.read()\n",
    "                    \n",
    "                        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "                        org = (20, 70)\n",
    "                        fontScale = 0.8\n",
    "                        color = (255, 0, 0)\n",
    "                        thickness = 2\n",
    "                        percentage = str(total*2) + \" %\"\n",
    "                    \n",
    "                        cv2.waitKey(1)\n",
    "                    \n",
    "                        count += 1\n",
    "                        actualTime = time.time()\n",
    "                        deltaNeeded = total*0.15\n",
    "                        deltaActual = actualTime - timeBase\n",
    "                        if deltaActual >= deltaNeeded:\n",
    "                            cv2.imwrite(os.path.join(\"../created_profiles/\", name + '/' + name + ' ' + str(total) + '.jpg') , frameCatch)\n",
    "                            total += 1\n",
    "                    \n",
    "                        frameOut = cv2.putText(frameCatch, percentage, org, font, fontScale, color, thickness, cv2.LINE_AA)\n",
    "                        frameCatch = cv2.cvtColor(frameCatch, cv2.COLOR_BGR2RGB)\n",
    "                        qimage = QImage(frameOut, frameOut.shape[1], frameOut.shape[0], QImage.Format_RGB888)\n",
    "                        pixmap = QPixmap(qimage)\n",
    "                        pixmap = pixmap.scaled(960,640, Qt.KeepAspectRatio)\n",
    "                        self.screen.setPixmap(pixmap)\n",
    "                    \n",
    "                    message = treatPictures(name)\n",
    "                    \n",
    "                    self.popup = QMessageBox(QMessageBox.Information,'Message',\"Pictures taken !\\n Now fiting ...\" + message)\n",
    "                    self.popup.show()\n",
    "                else:\n",
    "                    self.popup = QMessageBox(QMessageBox.Information,'Message','Please delete the existing folder.')\n",
    "                    self.popup.show()\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "    \n",
    "    def closeEvent(self, event): #Fonction qui se lance lors de la fermeture de la fenetre.\n",
    "        if self.camActivated:\n",
    "            self.cap.release()\n",
    "        else:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app = QApplication(sys.argv)\n",
    "window = MainWindow()\n",
    "window.show()\n",
    "app.exec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
